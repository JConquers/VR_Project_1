{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "from skimage.morphology import flood, binary_closing, disk\n",
    "from skimage import img_as_ubyte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows below is the part c of the project, until the demarcation where part d starts.\n",
    "______________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "#------------------------- NOTE : SET THE PATHS CORRECTLY IN THE NEXT CELL BEFORE PROCEDING ---------------------------------#\n",
    "\n",
    "Here for the part c of the project taht requires us to use traditional region-based segmenteaion techniques, we use the images available at <> where we have the correct corresponding mask so that our output can be validated>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '../MSFD/1/face_crop' # cropped faces with mask\n",
    "output_folder = '../output/part_c' # Path to the folder where you want the output segemented images to be saved\n",
    "ground_truth_folder= '../MSFD/1/face_crop_segmentation' # folder that contains actual face mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technique we choose is  clustering based region-based segmentation using the K-means algorithm.\n",
    "\n",
    "k=2, one for mask region and another for backround.\n",
    "\n",
    "Here for the choice of the 2 initial centroids, we use domain knowledge. The images are cropped to face-size which implies that it is higly likely that some region of the mask must be in the center of image. \n",
    "\n",
    "So we choose one centoid at center and another at corner.\n",
    "\n",
    "We try one more technique that is segmentaion by 'Region-growing', choice of initail seed here(only one) is center ofthe image, the 'why' of it backed by the reasoning provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_mask_region(image_path, viz = False, to_save = False, save_separetely = None):\n",
    "    # Load and preprocess image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 8)\n",
    "\n",
    "    # Compute Gradient for Additional Feature\n",
    "    sobelx = cv2.Sobel(blurred, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    sobely = cv2.Sobel(blurred, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    gradient_magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
    "    gradient_magnitude = cv2.normalize(gradient_magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Extract features (Intensity, Spatial Location, Gradient)\n",
    "    height, width = gray.shape\n",
    "    X_features = []\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            X_features.append([gray[y, x], x / width, y / height, gradient_magnitude[y, x]])\n",
    "\n",
    "    X_features = np.array(X_features)\n",
    "\n",
    "    # Manual Seed Selection\n",
    "    center_x, center_y = width // 2, height // 2\n",
    "    seed1 = [gray[center_y, center_x], center_x / width, center_y / height, gradient_magnitude[center_y, center_x]]\n",
    "    seed2 = [gray[0, 0], 0, 0, gradient_magnitude[0, 0]]  # Top-left corner as background\n",
    "\n",
    "    # Apply K-Means with Manual Seeds\n",
    "    kmeans = KMeans(n_clusters=2, init=np.array([seed1, seed2]), n_init=1, max_iter=100, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_features)\n",
    "   \n",
    "    # Invert if needed (as Mask should always be white)\n",
    "    #if kmeans.cluster_centers_[0][0] > kmeans.cluster_centers_[1][0]:  \n",
    "    labels = 1 - labels  # Swap labels\n",
    "\n",
    "    #print(kmeans.cluster_centers_)\n",
    "    #print(f'{type(labels)}, {len(labels)}')\n",
    "\n",
    "\n",
    "    # Reshaping the labels to match the image shape\n",
    "    segmented_image = labels.reshape((height, width))\n",
    "\n",
    "    # Convert segmented image to uint8 (0-255 range)\n",
    "    segmented_display = (segmented_image * 255).astype(np.uint8)\n",
    "\n",
    "    # Ensuring binary segmentation before Canny\n",
    "    segmented_display = cv2.threshold(segmented_display, 127, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # Morphological Closing to Refine Segmentation\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    segmented_display = cv2.morphologyEx(segmented_display, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Applying Canny Edge Detection\n",
    "    edges = cv2.Canny(segmented_display, 50, 150)\n",
    "\n",
    "    if(viz==True):\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        # Display Original Image\n",
    "        axes[0].imshow(image)\n",
    "        axes[0].set_title(\"Original Image\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        # Display Segmented Region (Fix Scaling Issue)\n",
    "        axes[1].imshow(segmented_display, cmap=\"gray\", vmin=0, vmax=255)\n",
    "        axes[1].set_title(\"Segmented Mask Region\")\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        # Display Edge Detection Result\n",
    "        axes[2].imshow(edges, cmap=\"gray\", vmin=0, vmax=255)\n",
    "        axes[2].set_title(\"Edges of Segmentation\")\n",
    "        axes[2].axis(\"off\")\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    if(to_save == True):\n",
    "        if(save_separetely != None):\n",
    "            cv2.imwrite(os.path.join(output_folder, image_path), segmented_display)\n",
    "        else:\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            cv2.imwrite(os.path.join(output_folder, image_path), segmented_display)\n",
    "\n",
    "    return segmented_display\n",
    "\n",
    "def segment_region_growing(image_path, tolerance=0.4, viz=False, to_save=False, save_path=None):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n",
    "    \n",
    "    # Convert to grayscale & apply Gaussian Blur\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (17, 17), 8)\n",
    "\n",
    "    # Normalize image to [0,1] for flood fill\n",
    "    norm_image = blurred.astype(np.float32) / 255.0  \n",
    "\n",
    "    # Define seed point at image center\n",
    "    height, width = norm_image.shape\n",
    "    seed_point = (height // 2, width // 2)\n",
    "    \n",
    "    # Perform region growing using flood fill\n",
    "    mask = flood(norm_image, seed_point, tolerance=tolerance)\n",
    "    \n",
    "    # Convert mask to uint8 binary image (0 or 255)\n",
    "    segmented_display = img_as_ubyte(mask)\n",
    "\n",
    "    # Apply morphological closing to refine segmentation\n",
    "    segmented_display = binary_closing(segmented_display, disk(3)).astype(np.uint8) * 255\n",
    "\n",
    "    # Detect edges using Canny\n",
    "    edges = cv2.Canny(segmented_display, 50, 150)\n",
    "\n",
    "    # Visualization\n",
    "    if viz:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        axes[0].imshow(image)\n",
    "        axes[0].set_title(\"Original Image\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        axes[1].imshow(segmented_display, cmap=\"gray\")\n",
    "        axes[1].set_title(\"Segmented Region\")\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        axes[2].imshow(edges, cmap=\"gray\")\n",
    "        axes[2].set_title(\"Edges of Segmentation\")\n",
    "        axes[2].axis(\"off\")\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    # Save result if needed\n",
    "    if to_save:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        cv2.imwrite(save_path, segmented_display)\n",
    "\n",
    "    return segmented_display\n",
    "\n",
    "def compute_metrics(pred_mask, true_mask_path):\n",
    "    true_mask = cv2.imread(true_mask_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
    "    if true_mask is None:\n",
    "        raise ValueError(f\"Error: Could not load {true_mask_path}\")\n",
    "\n",
    "    # Convert to binary masks\n",
    "    pred_mask = (pred_mask > 0).astype(np.uint8)\n",
    "    true_mask = (true_mask > 0).astype(np.uint8)\n",
    "\n",
    "    # Compute IoU\n",
    "    intersection = np.logical_and(pred_mask, true_mask).sum()\n",
    "    union = np.logical_or(pred_mask, true_mask).sum()\n",
    "    iou = intersection / union if union > 0 else 0\n",
    "\n",
    "    # Compute Dice\n",
    "    dice = (2 * intersection) / (pred_mask.sum() + true_mask.sum()) if (pred_mask.sum() + true_mask.sum()) > 0 else 0\n",
    "\n",
    "    return {\"IoU\": iou, \"Dice\": dice}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test on individual images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_mask = segment_mask_region('../MSFD/1/face_crop/000007_1.jpg', viz=True)\n",
    "pred_mask = segment_region_growing(image_path='../MSFD/1/face_crop/000007_1.jpg', viz=True, tolerance=0.19)\n",
    "true_mask_path = '../MSFD/1/face_crop_segmentation/000007_1.jpg'\n",
    "print(compute_metrics(pred_mask, true_mask_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we attemp to find average IOU and Dice scores across all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_iou = 0\n",
    "avg_dice = 0\n",
    "count = 0  # Count of valid images\n",
    "\n",
    "# Process each image\n",
    "for img in os.listdir(input_folder):\n",
    "    if img.lower().endswith(('.png', '.jpg', '.jpeg')):  # Filter images\n",
    "        img_path = os.path.join(input_folder, img)\n",
    "        gt_mask_path = os.path.join(ground_truth_folder, img)  # Assuming same filename\n",
    "\n",
    "        if not os.path.exists(gt_mask_path):\n",
    "            print(f\"Skipping {img} (No ground truth found)\")\n",
    "            continue\n",
    "        \n",
    "        predicted_segment = segment_region_growing(image_path=img_path, tolerance=0.15)  # Generate mask\n",
    "        \n",
    "        # Load ground truth mask\n",
    "        true_mask = cv2.imread(gt_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if true_mask is None:\n",
    "            print(f\"Skipping {img} (Cannot load ground truth mask)\")\n",
    "            continue\n",
    "        \n",
    "        # Check shape compatibility\n",
    "        if predicted_segment.shape != true_mask.shape:\n",
    "            print(f\"Skipping {img} (Shape mismatch: {predicted_segment.shape} vs {true_mask.shape})\")\n",
    "            continue\n",
    "        \n",
    "        metrics = compute_metrics(predicted_segment, gt_mask_path)  # Compute IoU & Dice\n",
    "        \n",
    "        if metrics:  # If computation was successful\n",
    "            avg_iou += metrics[\"IoU\"]\n",
    "            avg_dice += metrics[\"Dice\"]\n",
    "            count += 1  \n",
    "\n",
    "if count > 0:\n",
    "    avg_iou /= count\n",
    "    avg_dice /= count\n",
    "    print(f\"\\nAverage IoU: {avg_iou:.4f}, Average Dice: {avg_dice:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo valid images processed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part d\n",
    "______________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Val Metrics: {'Dice': np.float32(0.0020804573), 'IoU': np.float32(0.0011399745), 'Precision': np.float32(0.030992325)}\n",
      "Epoch 2/5 - Val Metrics: {'Dice': np.float32(0.020634636), 'IoU': np.float32(0.012148887), 'Precision': np.float32(0.15607879)}\n",
      "Epoch 3/5 - Val Metrics: {'Dice': np.float32(0.017132705), 'IoU': np.float32(0.010042136), 'Precision': np.float32(0.18905637)}\n",
      "Epoch 4/5 - Val Metrics: {'Dice': np.float32(0.036102004), 'IoU': np.float32(0.021313338), 'Precision': np.float32(0.34320235)}\n",
      "Epoch 5/5 - Val Metrics: {'Dice': np.float32(0.020815808), 'IoU': np.float32(0.012030196), 'Precision': np.float32(0.32748264)}\n",
      "Test Metrics: {'Dice': np.float32(0.035041634), 'IoU': np.float32(0.020639274), 'Precision': np.float32(0.35789925)}\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, data_loader, device, criterion=None):\n",
    "    model.eval()\n",
    "    dice_list, iou_list, prec_list = [], [], []\n",
    "    recall_list, f1_list = [], []\n",
    "    loss_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in data_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            logits = model(images)\n",
    "            preds = torch.sigmoid(logits) > 0.5\n",
    "\n",
    "            # Compute loss if a criterion is provided\n",
    "            if criterion is not None:\n",
    "                loss_val = criterion(logits, masks)\n",
    "                loss_list.append(loss_val.item())\n",
    "\n",
    "            intersection = (preds & (masks > 0.5)).float().sum(dim=(1,2,3))\n",
    "            union = (preds | (masks > 0.5)).float().sum(dim=(1,2,3))\n",
    "\n",
    "            dice = 2.0 * intersection / (preds.sum(dim=(1,2,3)) + masks.sum(dim=(1,2,3)) + 1e-8)\n",
    "            iou = intersection / (union + 1e-8)\n",
    "            precision = intersection / (preds.sum(dim=(1,2,3)) + 1e-8)\n",
    "            recall = intersection / ((masks > 0.5).float().sum(dim=(1,2,3)) + 1e-8)\n",
    "            f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "            dice_list.extend(dice.cpu().numpy())\n",
    "            iou_list.extend(iou.cpu().numpy())\n",
    "            prec_list.extend(precision.cpu().numpy())\n",
    "            recall_list.extend(recall.cpu().numpy())\n",
    "            f1_list.extend(f1.cpu().numpy())\n",
    "\n",
    "    metrics = {\n",
    "        \"Dice\": np.mean(dice_list),\n",
    "        \"IoU\": np.mean(iou_list),\n",
    "        \"Precision\": np.mean(prec_list),\n",
    "        \"Recall\": np.mean(recall_list),\n",
    "        \"F1\": np.mean(f1_list)\n",
    "    }\n",
    "    if criterion is not None and len(loss_list) > 0:\n",
    "        metrics[\"Loss\"] = np.mean(loss_list)\n",
    "    return metrics\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(SimpleUNet, self).__init__()\n",
    "        self.enc1 = nn.Sequential(nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(inplace=True))\n",
    "        self.enc2 = nn.Sequential(nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(inplace=True))\n",
    "        self.dec1 = nn.Sequential(nn.ConvTranspose2d(128, 64, 2, stride=2), nn.ReLU(inplace=True))\n",
    "        self.outc = nn.Conv2d(64, num_classes, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.enc1(x)\n",
    "        x2 = nn.MaxPool2d(2)(x1)\n",
    "        x2 = self.enc2(x2)\n",
    "        x3 = self.dec1(x2)\n",
    "        x_out = self.outc(x3)\n",
    "        return x_out\n",
    "\n",
    "class FaceMaskDataset(Dataset):\n",
    "    def __init__(self, img_dir, seg_dir, transform=None):\n",
    "        self.img_paths = sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir)])\n",
    "        self.seg_paths = sorted([os.path.join(seg_dir, f) for f in os.listdir(seg_dir)])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.img_paths[idx]).convert('RGB')\n",
    "        mask = Image.open(self.seg_paths[idx]).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        return image, mask\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "full_dataset = FaceMaskDataset(\n",
    "    img_dir=\"../MSFD/1/img\",\n",
    "    seg_dir=\"../MSFD/1/face_crop_segmentation\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Split into train/val/test (for example, 70/20/10)\n",
    "total_len = len(full_dataset)\n",
    "train_len = int(0.7 * total_len)\n",
    "val_len = int(0.2 * total_len)\n",
    "test_len = total_len - train_len - val_len\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_len, val_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "model = SimpleUNet(num_classes=1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 5\n",
    "best_val_dice = 0.0\n",
    "best_model_path = \"best_face_mask_unet.pth\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_metrics = evaluate_model(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Val Metrics:\", val_metrics)\n",
    "    if val_metrics[\"Dice\"] > best_val_dice:\n",
    "        best_val_dice = val_metrics[\"Dice\"]\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "# Load best model and compute final test metrics\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "test_metrics = evaluate_model(model, test_loader, device)\n",
    "print(\"Test Metrics:\", test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Epoch 1/10 - Val Metrics: {'Dice': np.float32(0.049665358), 'IoU': np.float32(0.029051635), 'Precision': np.float32(0.41305828), 'Recall': np.float32(0.034094), 'F1': np.float32(0.049328446)}\n",
      "Extended Epoch 2/10 - Val Metrics: {'Dice': np.float32(0.018553255), 'IoU': np.float32(0.010536993), 'Precision': np.float32(0.3385195), 'Recall': np.float32(0.011891935), 'F1': np.float32(0.018424)}\n",
      "Extended Epoch 3/10 - Val Metrics: {'Dice': np.float32(0.042415082), 'IoU': np.float32(0.02445732), 'Precision': np.float32(0.40350398), 'Recall': np.float32(0.028059736), 'F1': np.float32(0.042117607)}\n",
      "Extended Epoch 4/10 - Val Metrics: {'Dice': np.float32(0.019119015), 'IoU': np.float32(0.010661044), 'Precision': np.float32(0.35684618), 'Recall': np.float32(0.011836304), 'F1': np.float32(0.01898374)}\n",
      "Extended Epoch 5/10 - Val Metrics: {'Dice': np.float32(0.023304809), 'IoU': np.float32(0.013222252), 'Precision': np.float32(0.36518505), 'Recall': np.float32(0.014911198), 'F1': np.float32(0.023137102)}\n",
      "Extended Epoch 6/10 - Val Metrics: {'Dice': np.float32(0.022878824), 'IoU': np.float32(0.013024724), 'Precision': np.float32(0.33445868), 'Recall': np.float32(0.014690063), 'F1': np.float32(0.022716781)}\n",
      "Extended Epoch 7/10 - Val Metrics: {'Dice': np.float32(0.06540259), 'IoU': np.float32(0.03775967), 'Precision': np.float32(0.4423216), 'Recall': np.float32(0.0439329), 'F1': np.float32(0.0649675)}\n",
      "Extended Epoch 8/10 - Val Metrics: {'Dice': np.float32(0.04058629), 'IoU': np.float32(0.023161111), 'Precision': np.float32(0.4233932), 'Recall': np.float32(0.026104683), 'F1': np.float32(0.040305357)}\n",
      "Extended Epoch 9/10 - Val Metrics: {'Dice': np.float32(0.043201566), 'IoU': np.float32(0.02452731), 'Precision': np.float32(0.43277672), 'Recall': np.float32(0.02754113), 'F1': np.float32(0.042898208)}\n",
      "Extended Epoch 10/10 - Val Metrics: {'Dice': np.float32(0.050765026), 'IoU': np.float32(0.029320072), 'Precision': np.float32(0.42011428), 'Recall': np.float32(0.033698045), 'F1': np.float32(0.050422475)}\n",
      "New Test Metrics: {'Dice': np.float32(0.067108035), 'IoU': np.float32(0.038643308), 'Precision': np.float32(0.45882922), 'Recall': np.float32(0.044504028), 'F1': np.float32(0.06667898)}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, device, criterion=None):\n",
    "    model.eval()\n",
    "    dice_list, iou_list, prec_list = [], [], []\n",
    "    recall_list, f1_list = [], []\n",
    "    loss_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in data_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            logits = model(images)\n",
    "            preds = torch.sigmoid(logits) > 0.5\n",
    "\n",
    "            # Compute loss if a criterion is provided\n",
    "            if criterion is not None:\n",
    "                loss_val = criterion(logits, masks)\n",
    "                loss_list.append(loss_val.item())\n",
    "\n",
    "            intersection = (preds & (masks > 0.5)).float().sum(dim=(1,2,3))\n",
    "            union = (preds | (masks > 0.5)).float().sum(dim=(1,2,3))\n",
    "\n",
    "            dice = 2.0 * intersection / (preds.sum(dim=(1,2,3)) + masks.sum(dim=(1,2,3)) + 1e-8)\n",
    "            iou = intersection / (union + 1e-8)\n",
    "            precision = intersection / (preds.sum(dim=(1,2,3)) + 1e-8)\n",
    "            recall = intersection / ((masks > 0.5).float().sum(dim=(1,2,3)) + 1e-8)\n",
    "            f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "            dice_list.extend(dice.cpu().numpy())\n",
    "            iou_list.extend(iou.cpu().numpy())\n",
    "            prec_list.extend(precision.cpu().numpy())\n",
    "            recall_list.extend(recall.cpu().numpy())\n",
    "            f1_list.extend(f1.cpu().numpy())\n",
    "\n",
    "    metrics = {\n",
    "        \"Dice\": np.mean(dice_list),\n",
    "        \"IoU\": np.mean(iou_list),\n",
    "        \"Precision\": np.mean(prec_list),\n",
    "        \"Recall\": np.mean(recall_list),\n",
    "        \"F1\": np.mean(f1_list)\n",
    "    }\n",
    "    if criterion is not None and len(loss_list) > 0:\n",
    "        metrics[\"Loss\"] = np.mean(loss_list)\n",
    "    return metrics\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Continue training for 10 more epochs\n",
    "more_epochs = 10\n",
    "for epoch in range(more_epochs):\n",
    "    model.train()\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    val_metrics = evaluate_model(model, val_loader, device)\n",
    "    print(f\"Extended Epoch {epoch + 1}/{more_epochs} - Val Metrics:\", val_metrics)\n",
    "    if val_metrics[\"Dice\"] > best_val_dice:\n",
    "        best_val_dice = val_metrics[\"Dice\"]\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "# Test with the new best model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "test_metrics2 = evaluate_model(model, test_loader, device)\n",
    "print(\"New Test Metrics:\", test_metrics2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Conv2DTranspose, Concatenate,\n",
    "                                     Dropout)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example placeholders for images/masks arrays:\n",
    "# images, masks = load_your_data()  # (N, H, W, 3), (N, H, W, 1) for example\n",
    "\n",
    "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_f = tf.keras.backend.flatten(tf.cast(y_true, tf.float32))\n",
    "    y_pred_f = tf.keras.backend.flatten(tf.cast(y_pred, tf.float32))\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (\n",
    "        tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth\n",
    "    )\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.BinaryCrossentropy()(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "def unet(input_shape, output_layer=1):\n",
    "    inputs = Input(input_shape)\n",
    "    # Encoder\n",
    "    c1 = Conv2D(16, (3,3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(16, (3,3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2,2))(c1)\n",
    "\n",
    "    c2 = Conv2D(32, (3,3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(32, (3,3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2,2))(c2)\n",
    "\n",
    "    c3 = Conv2D(64, (3,3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(64, (3,3), activation='relu', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2,2))(c3)\n",
    "\n",
    "    c4 = Conv2D(128, (3,3), activation='relu', padding='same')(p3)\n",
    "    c4 = Conv2D(128, (3,3), activation='relu', padding='same')(c4)\n",
    "    p4 = MaxPooling2D((2,2))(c4)\n",
    "\n",
    "    # Bottom\n",
    "    c5 = Conv2D(256, (3,3), activation='relu', padding='same')(p4)\n",
    "    c5 = Conv2D(256, (3,3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    # Decoder\n",
    "    u6 = Conv2DTranspose(128, (2,2), strides=(2,2), padding='same')(c5)\n",
    "    x6 = Concatenate()([c4, u6])\n",
    "    x6 = Dropout(0.2)(x6)\n",
    "    c6 = Conv2D(128, (3,3), activation='relu', padding='same')(x6)\n",
    "    c6 = Conv2D(128, (3,3), activation='relu', padding='same')(c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(64, (2,2), strides=(2,2), padding='same')(c6)\n",
    "    x7 = Concatenate()([c3, u7])\n",
    "    x7 = Dropout(0.2)(x7)\n",
    "    c7 = Conv2D(64, (3,3), activation='relu', padding='same')(x7)\n",
    "    c7 = Conv2D(64, (3,3), activation='relu', padding='same')(c7)\n",
    "\n",
    "    u8 = Conv2DTranspose(32, (2,2), strides=(2,2), padding='same')(c7)\n",
    "    x8 = Concatenate()([c2, u8])\n",
    "    x8 = Dropout(0.1)(x8)\n",
    "    c8 = Conv2D(32, (3,3), activation='relu', padding='same')(x8)\n",
    "    c8 = Conv2D(32, (3,3), activation='relu', padding='same')(c8)\n",
    "\n",
    "    u9 = Conv2DTranspose(16, (2,2), strides=(2,2), padding='same')(c8)\n",
    "    x9 = Concatenate()([c1, u9])\n",
    "    x9 = Dropout(0.1)(x9)\n",
    "    c9 = Conv2D(16, (3,3), activation='relu', padding='same')(x9)\n",
    "    c9 = Conv2D(16, (3,3), activation='relu', padding='same')(c9)\n",
    "\n",
    "    outputs = Conv2D(output_layer, (1,1), activation='sigmoid')(c9)\n",
    "    return Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "# Example data splitting\n",
    "# images, masks = load_images(), load_masks() # shape e.g. (N, 256, 256, 3), (N, 256, 256, 1)\n",
    "# train_images, val_images, train_masks, val_masks = train_test_split(images, masks, test_size=0.2, random_state=87)\n",
    "# train_images, test_images, train_masks, test_masks = train_test_split(train_images, train_masks, test_size=0.2, random_state=87)\n",
    "\n",
    "model = unet((256,256,3))\n",
    "optimizer = Adam(learning_rate=2e-4)\n",
    "model.compile(optimizer=optimizer, loss=bce_dice_loss, metrics=[dice_coefficient])\n",
    "\n",
    "# Example training\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "# checkpoint = ModelCheckpoint(\"best.h5\", monitor=\"val_dice_coefficient\", save_best_only=True, mode=\"max\")\n",
    "# lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, mode=\"min\", verbose=1)\n",
    "# history = model.fit(\n",
    "#     train_images, train_masks,\n",
    "#     batch_size=4,\n",
    "#     epochs=20,\n",
    "#     validation_data=(val_images, val_masks),\n",
    "#     callbacks=[checkpoint, lr_scheduler]\n",
    "# )\n",
    "# model.load_weights(\"best.h5\")\n",
    "# test_results = model.evaluate(test_images, test_masks, verbose=0)\n",
    "# print(\"Test Results:\", test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part d [continued....]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.image import flip_left_right, flip_up_down, adjust_brightness\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and Preprocessing\n",
    "\n",
    "In this section, we define functions to load and preprocess the data. The `load_data` function retrieves image filenames from the specified folder, while the `preprocess_data` function resizes the images and masks to the desired input size and applies optional data augmentation techniques such as flipping.\n",
    "\n",
    "The preprocessed data is then split into training, validation, and test sets for model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "def load_data(folder_path):\n",
    "    folder=os.listdir(folder_path)\n",
    "    images=[]\n",
    "    for filename in folder:\n",
    "        if(filename not in [\"000601_1.jpg\"]):\n",
    "            images.append(filename)\n",
    "    images = sorted(images)[:6000]\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change paths to the correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change path\n",
    "image_filenames=load_data(\"../MSFD/1/face_crop\") \n",
    "mask_filenames = load_data(\"../MSFD/1/face_crop_segmentation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "### Display Data Function\n",
    "\n",
    "The `display_data` function is designed to visualize pairs of images and their corresponding masks. It takes the following parameters:\n",
    "\n",
    "- `actual_img`: Path to the folder containing the original images.\n",
    "- `mask_img`: Path to the folder containing the mask images.\n",
    "- `image_paths`: List of filenames for the images.\n",
    "- `mask_paths`: List of filenames for the masks.\n",
    "\n",
    "#### Functionality:\n",
    "1. Creates a grid of subplots with 5 rows and 2 columns to display 5 image-mask pairs.\n",
    "2. Iterates through the provided image and mask paths:\n",
    "    - Reads the image and mask using `plt.imread`.\n",
    "    - Displays the image in the left column and the corresponding mask in the right column.\n",
    "3. Adjusts the layout for better spacing using `plt.tight_layout`.\n",
    "4. Displays the plot using `plt.show`.\n",
    "\n",
    "This function is useful for visually inspecting the dataset to ensure the images and masks are correctly aligned and formatted.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_data(actual_img, mask_img, image_paths, mask_paths):\n",
    "\n",
    "    fig, axes = plt.subplots(5, 2, figsize=(10, 20))\n",
    "    for i, (image_path, mask_path) in enumerate(zip(image_paths, mask_paths)):\n",
    "        image = plt.imread(actual_img + image_path)\n",
    "        mask = plt.imread(mask_img + mask_path)\n",
    "        print(image_path, mask_path)\n",
    "        axes[i, 0].imshow(image)\n",
    "        axes[i, 0].set_title('Image')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        axes[i, 1].imshow(mask)\n",
    "        axes[i, 1].set_title('Mask')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "### Preprocessing Function\n",
    "\n",
    "The `preprocess_data` function is designed to prepare image and mask data for training and evaluation. It performs the following steps:\n",
    "\n",
    "#### Parameters:\n",
    "- `actual_img`: Path to the folder containing the original images.\n",
    "- `mask_img`: Path to the folder containing the mask images.\n",
    "- `image_names`: List of filenames for the images.\n",
    "- `mask_names`: List of filenames for the masks.\n",
    "- `input_size`: Tuple specifying the target size for resizing the images and masks (e.g., `(256, 256)`).\n",
    "- `augmented`: Boolean flag to indicate whether data augmentation (flipping) should be applied.\n",
    "\n",
    "#### Functionality:\n",
    "1. **Image and Mask Loading**:\n",
    "    - Reads each image and its corresponding mask using `load_img`.\n",
    "    - Resizes them to the specified `input_size`.\n",
    "    - Converts the images to RGB and masks to grayscale.\n",
    "\n",
    "2. **Normalization**:\n",
    "    - Normalizes the image pixel values to the range `[0, 1]`.\n",
    "    - Converts the mask to a boolean array.\n",
    "\n",
    "3. **Data Augmentation** (if `augmented=True`):\n",
    "    - Applies horizontal and vertical flipping to both images and masks to increase dataset diversity.\n",
    "\n",
    "4. **Conversion to Numpy Arrays**:\n",
    "    - Converts the lists of images and masks into numpy arrays for efficient processing.\n",
    "\n",
    "#### Returns:\n",
    "- `images`: Numpy array of preprocessed images.\n",
    "- `masks`: Numpy array of preprocessed masks.\n",
    "\n",
    "This function ensures that the data is properly formatted and optionally augmented for training deep learning models.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "def preprocess_data(actual_img, mask_img, image_names, mask_names, input_size, augmented=False):\n",
    "\n",
    "    images = []\n",
    "    masks = []\n",
    "    for img_file, mask_file in zip(image_names, mask_names):\n",
    "        img = load_img(actual_img + img_file, target_size=input_size, color_mode='rgb')\n",
    "        mask = load_img(mask_img + mask_file, target_size=input_size, color_mode='grayscale')\n",
    "\n",
    "        # Convert image and mask to arrays\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = img_array / 255.0\n",
    "\n",
    "        mask_array = img_to_array(mask, dtype=np.bool_)\n",
    "\n",
    "        # Append images and masks to the lists\n",
    "        images.append(img_array)\n",
    "        masks.append(mask_array)\n",
    "\n",
    "        if augmented:\n",
    "            images.append(flip_left_right(img_array))\n",
    "            masks.append(flip_left_right(mask_array))\n",
    "\n",
    "            images.append(flip_up_down(img_array))\n",
    "            masks.append(flip_up_down(mask_array))\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    images = np.array(images)\n",
    "    masks = np.array(masks)\n",
    "\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change to correct path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (256, 256)\n",
    "INPUT_SHAPE = (256, 256, 3)\n",
    "\n",
    "images,masks=preprocess_data(\"../MSFD/1/face_crop/\",\"../MSFD/1/face_crop_segmentation/\",image_filenames,mask_filenames,INPUT_SIZE)\n",
    "print('Shape of image data: ' + str(images.shape))\n",
    "print('Shape of mask data: ' + str(masks.shape))\n",
    "# Split the dataset into training and validation sets\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(images, masks, test_size=0.1, random_state=87)\n",
    "train_images, test_images, train_masks, test_masks = train_test_split(train_images, train_masks, test_size=0.1, random_state=87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=1/5, patience=3, verbose=1)\n",
    "checkpoint = tf.kerasa.callbacks.ModelCheckpoint('save_best.keras', verbose=1, save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to calculate dice score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)  # Convert y_true to float32\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.BinaryCrossentropy()(y_true, y_pred) + dice_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net Model Implementation\n",
    "\n",
    "The following code defines a U-Net model architecture using TensorFlow and Keras. U-Net is a popular convolutional neural network architecture for image segmentation tasks.\n",
    "\n",
    "Key Components:\n",
    "1. **Imports**:\n",
    "    - TensorFlow and Keras modules are imported for building the model.\n",
    "    - Layers such as `Conv2D`, `MaxPooling2D`, `Dropout`, `UpSampling2D`, `Concatenate`, and `Conv2DTranspose` are used to construct the U-Net.\n",
    "\n",
    "2. **Function Definition**:\n",
    "    - The `unet` function takes two parameters:\n",
    "      - `input_shape`: The shape of the input images (e.g., `(256, 256, 3)` for RGB images).\n",
    "      - `output_layer`: The number of output channels (e.g., `1` for binary segmentation).\n",
    "\n",
    "3. **Architecture**:\n",
    "    - **Encoder**:\n",
    "      - Consists of convolutional layers (`Conv2D`) followed by max-pooling layers (`MaxPooling2D`).\n",
    "      - Each block doubles the number of filters, starting from 16.\n",
    "    - **Bottleneck**:\n",
    "      - The deepest part of the network with the highest number of filters (256).\n",
    "    - **Decoder**:\n",
    "      - Uses transposed convolutional layers (`Conv2DTranspose`) for upsampling.\n",
    "      - Skip connections (`Concatenate`) are used to combine features from the encoder with the decoder.\n",
    "\n",
    "4. **Output**:\n",
    "    - The final layer uses a `Conv2D` layer with a sigmoid activation function to produce the segmentation mask.\n",
    "\n",
    "Usage:\n",
    "- Call the `unet` function with the desired input shape and output layer to create the model.\n",
    "- Compile the model with an appropriate loss function and optimizer before training.\n",
    "```python\n",
    "model = unet((256, 256, 3), output_layer=1)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, UpSampling2D, Concatenate, Conv2DTranspose\n",
    "\n",
    "def unet(input_shape, output_layer):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "    #conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv1)\n",
    "    #conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "    #conv1 = Dropout(0.1)(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool1)\n",
    "    #conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv2)\n",
    "    #conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    #conv2 = Dropout(0.1)(conv2)\n",
    "\n",
    "    conv3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool2)\n",
    "    #conv3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv3)\n",
    "    #conv3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    #conv3 = Dropout(0.2)(conv3)\n",
    "\n",
    "    conv4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool3)\n",
    "    #conv4 = tf.keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv4)\n",
    "    #conv4 = tf.keras.layers.BatchNormalization()(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    #conv4 = Dropout(0.2)(conv4)\n",
    "\n",
    "    # Bottom\n",
    "    conv5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool4)\n",
    "    #conv5 = tf.keras.layers.BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv5)\n",
    "    #conv5 = tf.keras.layers.BatchNormalization()(conv5)\n",
    "    #conv5 = Dropout(0.3)(conv5)\n",
    "\n",
    "    # Decoder\n",
    "    up6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv5)\n",
    "    merge6 = Concatenate()([conv4, up6])\n",
    "    conv6 = Dropout(0.2)(merge6)\n",
    "    conv6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv6)\n",
    "    #conv6 = tf.keras.layers.BatchNormalization()(conv6)\n",
    "    conv6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv6)\n",
    "    #conv6 = tf.keras.layers.BatchNormalization()(conv6)\n",
    "\n",
    "    up7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv6)\n",
    "    merge7 = Concatenate()([conv3, up7])\n",
    "    conv7 = Dropout(0.2)(merge7)\n",
    "    conv7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv7)\n",
    "    #conv7 = tf.keras.layers.BatchNormalization()(conv7)\n",
    "    conv7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv7)\n",
    "    #conv7 = tf.keras.layers.BatchNormalization()(conv7)\n",
    "\n",
    "    up8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv7)\n",
    "    merge8 = Concatenate()([conv2, up8])\n",
    "    conv8 = Dropout(0.1)(merge8)\n",
    "    conv8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv8)\n",
    "    #conv8 = tf.keras.layers.BatchNormalization()(conv8)\n",
    "    conv8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv8)\n",
    "    #conv8 = tf.keras.layers.BatchNormalization()(conv8)\n",
    "\n",
    "    up9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(conv8)\n",
    "    merge9 = Concatenate()([conv1, up9])\n",
    "    conv9 = Dropout(0.1)(merge9)\n",
    "    conv9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv9)\n",
    "    #conv9 = tf.keras.layers.BatchNormalization()(conv9)\n",
    "    conv9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv9)\n",
    "    #conv9 = tf.keras.layers.BatchNormalization()(conv9)\n",
    "\n",
    "    # Output\n",
    "    output = Conv2D(output_layer, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compiling model and setting learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet(INPUT_SHAPE, output_layer=1)\n",
    "optimizer = Adam(learning_rate=2e-4)\n",
    "\n",
    "# Compile the model with the optimizer instance\n",
    "model.compile(optimizer=optimizer, loss=bce_dice_loss, metrics=[dice_coefficient])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "history = model.fit(train_images, train_masks, batch_size=4, epochs=epochs, validation_data=(val_images, val_masks), callbacks=[checkpoint,lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model being stored so that we don't have to train again and again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the best saved model after training\n",
    "best_model = load_model('save_best.keras',custom_objects={'bce_dice_loss': bce_dice_loss, 'dice_coefficient': dice_coefficient})\n",
    "best_model.save('best_model.h5')\n",
    "eval = best_model.evaluate(test_images, test_masks)\n",
    "print('Test accuracy: ' + \"{:.2f}\".format(eval[1]))\n",
    "\n",
    "# load the saved model due to prior interuption\n",
    "model = load_model('best_model.h5',custom_objects={'bce_dice_loss': bce_dice_loss, 'dice_coefficient': dice_coefficient})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "showing our results on te images, 5 random iamges are being shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display 5 random predictions\n",
    "random.seed(87)\n",
    "predictions = model.predict(test_images)\n",
    "predictions = (predictions > 0.5).astype(np.uint8)\n",
    "\n",
    "fig, axes = plt.subplots(5, 4, figsize=(5, 3*5))\n",
    "\n",
    "# Iterate over the image and mask pairs and display them in subplots\n",
    "for i in range(5):\n",
    "\n",
    "    image = (test_images[i] * 255).astype(np.uint8)\n",
    "    mask = predictions[i]\n",
    "    ground_truth = test_masks[i] * np.array([255, 255, 255]) # convert the forground into yellow color to achieve the desired aesthetic\n",
    "\n",
    "    overlay = image.copy()\n",
    "\n",
    "    mask = np.repeat(mask, 3, axis=2) # matching the size of the channel of the mask and the image to perform an overlay\n",
    "    inverted_mask = 1 - mask\n",
    "\n",
    "    yellow_mask = np.array([255, 255, 255]) * mask\n",
    "\n",
    "    # Apply the mask on the image\n",
    "    result = image * inverted_mask + yellow_mask\n",
    "    alpha = 0.2\n",
    "    predicted_overlay = cv2.addWeighted(overlay, alpha, result.astype(overlay.dtype), 1 - alpha, 0)\n",
    "\n",
    "    # Plot the image and mask in the corresponding subplot\n",
    "    axes[i, 0].imshow(image)\n",
    "    axes[i, 0].set_title('Original')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    axes[i, 1].imshow(ground_truth)\n",
    "    axes[i, 1].set_title('Ground Truth')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    axes[i, 2].imshow(yellow_mask)\n",
    "    axes[i, 2].set_title('Predicted')\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    axes[i, 3].imshow(predicted_overlay)\n",
    "    axes[i, 3].set_title('Predicted Overlay')\n",
    "    axes[i, 3].axis('off')\n",
    "    \n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/result.png', bbox_inches='tight')  # Save as PNG image\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict(test_images)\n",
    "predictions = (predictions > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "showing model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, jaccard_score\n",
    "\n",
    "def flatten_masks(masks):\n",
    "    return masks.reshape(masks.shape[0], -1)  # Flatten each image in the batch\n",
    "\n",
    "def dice_score(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    return (2.0 * intersection) / (np.sum(y_true) + np.sum(y_pred) + 1e-7)  # Add small epsilon to avoid division by zero\n",
    "\n",
    "# Flatten the masks and predictions\n",
    "flat_predictions = flatten_masks(predictions)\n",
    "flat_test_masks = flatten_masks(test_masks)\n",
    "\n",
    "total_accuracy = 0\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "total_iou = 0\n",
    "total_dice = 0\n",
    "num_samples = predictions.shape[0]\n",
    "\n",
    "# Loop through each sample and compute the metrics\n",
    "for i in range(num_samples):\n",
    "    y_true = flat_test_masks[i]\n",
    "    y_pred = flat_predictions[i]\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    iou = jaccard_score(y_true, y_pred, zero_division=0)\n",
    "    dice = dice_score(y_true, y_pred)\n",
    "    \n",
    "    total_accuracy += accuracy\n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    total_iou += iou\n",
    "    total_dice += dice\n",
    "\n",
    "# Compute average metrics\n",
    "avg_accuracy = total_accuracy / num_samples\n",
    "avg_precision = total_precision / num_samples\n",
    "avg_recall = total_recall / num_samples\n",
    "avg_iou = total_iou / num_samples\n",
    "avg_dice = total_dice / num_samples\n",
    "\n",
    "# Print the results\n",
    "print(f'Average Accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average Precision: {avg_precision:.4f}')\n",
    "print(f'Average Recall: {avg_recall:.4f}')\n",
    "print(f'Average IoU: {avg_iou:.4f}')\n",
    "print(f'Average Dice Score: {avg_dice:.4f}')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
